{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Natural Language Processing\n",
    "- Evan Kramer\n",
    "- evankram\n",
    "- 10/7/2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "You'll be creating models to predict the reviews that lead to recommendations for the product, 5-star ratings, and reviews considered helpful, which are found in the field `reviews.doRecommend`, `reviews.rating`, and `reviews.numHelpful`, respectively. Create summary statistics and histograms for each of these fields. Do you see any issues in using these fields as outcome (target) variables? (3 points)\n",
    "- I first conducted some exploratory data analysis to determine whether there were any issues in using these fields to predict 5-star ratings. Here is what I found.\n",
    "    - There are no missing values, which means we won't have to impute data.\n",
    "    - However, this is a classification task (i.e., \"Did the reviewer give the Kindle a five-star rating or not?\") on a continuous variable (`reviews.rating`).\n",
    "        - As such, we will need to turn this into a categorical or boolean variable.\n",
    "    - Furthermore, there are unbalanced classes in the data.\n",
    "        - As the histograms show, more than three-quarters of reviews give a five-star rating (so a model that always predicts a five-star rating would be accurate about 76% of the time but would have poor recall).\n",
    "        - Similarly, the `reviews.numHelpful` variable has a mode of 0 and a long right tail, meaning that it has a skewed distribution. \n",
    "    - We can address these unbalanced classes by sampling the data and through cross-validation. More specifically: \n",
    "        - We can up-sample (i.e., add duplicate or synthetic observations) for the class that occurs less frequently.\n",
    "        - We can down-sample (i.e., remove) for the class that occurs more frequently.\n",
    "    - Normalizing the data does not seem to make much sense here given the tasks and the target variable (i.e., predict reviews that **two** or more people will find helpful and predict **five-star** ratings).\n",
    "        - That is, these numerical objectives would become harder to interpret in our model results. In addition, this would not help address the unbalanced classes we have.\n",
    "- Given all this, I: \n",
    "    - Added a boolean `reviews.fiveStarRating` variable that \n",
    "    - Added a `fold` variable in preparation of cross-validation and/or creating training/test splits \n",
    "        - **Note**: We can use both the `reviews.fiveStarRating` and `fold` variables if we're worried that our training and test splits \n",
    "        - **Note** sci-kit learn has built-in methods for cross-validating, so we can also use the `fold` variable to create simple train/test splits \n",
    "    - Decicided that up-sampling and down-sampling was risky here given that we would be looking text features as predictors.\n",
    "        - Furthermore, decision trees typically perform fairly well even with unbalanced classes, so long as we use the area under the ROC curve as our outcome of interest.\n",
    "        - Out of curiosity, though, I did create down-sampled and up-sampled versions of the data to see how the model results differed for those data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 844 entries, 0 to 843\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype\n",
      "---  ------               --------------  -----\n",
      " 0   reviews.doRecommend  844 non-null    bool \n",
      " 1   reviews.rating       844 non-null    int64\n",
      " 2   reviews.numHelpful   844 non-null    int64\n",
      "dtypes: bool(1), int64(2)\n",
      "memory usage: 14.1 KB\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXVUlEQVR4nO3df5BdZ33f8fcHydjYQGzjtSMkFZmpCrEZECAUUxMC2GAFCPI/TuUMHYXxVGHqNtAhSSXalJBGM+40Q0kmcRqFX2oBO8JgrJAEUEVwE8LYrI0Nlm1hBRlrIyEtDsbBFAWJb/+4j8LVan9caXe9u4f3a2bnnPOc55zzPVfSZ4+ee+85qSokSd3ylLkuQJI08wx3Seogw12SOshwl6QOMtwlqYMMd0nqIMNdcyrJ/0zyG3Ndx3yX5DeTfHjAvknywSTfTnLnAP0/lOS3p1+l5hPDXXOqqt5aVf91ruuYbUkeTnLlmLZfSvLXs3C4VwCvBZZV1ZpZ2L8WAMNd05Zk8VzXoBM8B3i4qp6Y60I0dwx3nZZ2Jfofk3wFeCLJK5L8TZLHktyb5FWt3/okw2O2/Q9JdrT5E4YEkrwxyT1tP3+T5IWt/S1J/rSv394k2/uW9ydZ1YYk/keSw0m+k+QrSV4wyTn8auvznSR/kuSstu6kq+okleSf99V9Y5K/SPLdJF9I8pNJ3tuGQx5M8uJTfE2fneTjSUaT7EvyKxP0W9Fq2ZjkQJKDSd7R1l0HvA94eavr3VOdi7rJcNd0XAu8AXgucBvw28D5wK8CH08yBOwAnpdkZd92vwh8dOzOkrwE+ADwy8CzgD8CdiQ5E7gd+JkkT0myBDgDuLxt91zg6cBXgNcBrwT+BXAu8K+ARyc5h18A1gIXAy8EfukUzv8XgP8MXAAcAb4I3N2WbwHeM+iOkjwF+FPgXmApcAXw9iRXTbLZq4GV9M55U5Irq+r9wFuBL1bV06vqXadwPuoQw13T8XtVtR94M/DnVfXnVfXDqtoJDAOvr6rv0Qv+awFayD+fXuiP9W+AP6qqO6rqWFVtoxeal1XV14F/AFYBPwt8Bvi7JM9vy39VVT8EfgA8ox0jVfVAVR2c4hwOVNXf0wvXVadw/rdW1V1V9X3gVuD7VfW/quoY8CfA2Cv3T7b/kTyW5DHgxr51LwOGquq3quof2/n+MbB+kuO/u6qeqKqvAh+kvcYSGO6anv1t+hzgmjHB9QpgSVv/UX4UPL8IfLKF/ljPAd4xZj/LgWe39bcDr6J3ZX478Hl6wf6zbZmq+hzw+8AfAIeSbE3yzEnO4Zt989+j9z+AQR3qm/9/4yyP3dfVVXXu8R/g3/atew7w7DHn/k7gokmOv79v/hv86HWSDHdNy/Fbiu4H/nd/cFXVOVV1Q1v/WeCCJKvohfxJQzJ9+9kyZj9nV9VNbf3xcP+ZNn87Y8IdoKp+r6peClxKb3jm107j3J4Azj6+kOQnT2Mfp2I/sG/MuT+jql4/yTbL++b/GXBggn5P9rloHjDcNRM+DPx8kquSLEpyVpJXJVkGUFVH6Y1B/3d6Y/I7J9jPHwNvTfLT7Y3Rc5K8Ickz2vrb6Y0zP62qRoC/ojde/izgywBJXta2P4NeqH0fOHYa53QvcGl7k/Ys4DdPYx+n4k7g8fYm9dPa6/iCJC+bZJvfSHJ2kkuBt9AbChrPk30umgcMd01bG3dfR28YYZTeVeivceLfr48CVwIfa2E/3n6G6Y27/z7wbWAvfW9wVtXXgO/SC3Wq6nHg68AX2jg3wDPp/ZL4Nr2hikeB3wFI8s4kfzHgOX0N+C3g/wAPAbPxefT+4x0Dfp7emP8+4Fv0PvXyE5Nsdju912gX8DtV9dkJ9v2knovmh/iwDmlhSbKC3i+AMyb6RSl55S5JHWS4S1IHOSwjSR3klbskddC8uOHTBRdcUCtWrJjrMiRpQbnrrru+VVVD462bF+G+YsUKhoeHp+4oSfonSb4x0TqHZSSpgwx3Seogw12SOshwl6QOMtwlqYMGCvf2WLTdSe5LclO769/5SXYmeahNz+vrv7k9Bm3PFE+SkSTNginDPclS4FeA1VX1AmARvafDbAJ2VdVKenel29T6X9LWX0rvdqw3Jlk0O+VLksYz6LDMYuBp6T3l/mx6DwVYB2xr67cBV7f5dcDNVXWkqvbRuyXpmhmrWJI0pSnDvar+jt79sB8BDgLfafeNvuj4synb9MK2yVJOfPzXSGs7QXty+3CS4dHR0emdhSTpBFN+Q7WNpa+j93T4x4CPJXnzZJuM03bS3cmqaiuwFWD16tXTunvZik1/Np3NT9vDN7xhTo4rSVMZZFjmSnrPdhytqh8AnwD+Jb2HDy8BaNPDrf8IJz7bcRkTP9tRkjQLBgn3R4DL2rMaA1wBPADsADa0PhuA29r8DmB9kjOTXAyspPd8SEnSk2TKYZmquiPJLcDdwFF6DyLeCjwd2J7kOnq/AK5p/Xcn2Q7c3/pf3/d8S0nSk2Cgu0JW1buAd41pPkLvKn68/luALdMrTZJ0uvyGqiR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskddCU4Z7keUnu6ft5PMnbk5yfZGeSh9r0vL5tNifZm2RPkqtm9xQkSWNNGe5VtaeqVlXVKuClwPeAW4FNwK6qWgnsasskuQRYD1wKrAVuTLJodsqXJI3nVIdlrgD+tqq+AawDtrX2bcDVbX4dcHNVHamqfcBeYM0M1CpJGtCphvt64KY2f1FVHQRo0wtb+1Jgf982I63tBEk2JhlOMjw6OnqKZUiSJjNwuCd5KvAm4GNTdR2nrU5qqNpaVauravXQ0NCgZUiSBnAqV+4/B9xdVYfa8qEkSwDa9HBrHwGW9223DDgw3UIlSYM7lXC/lh8NyQDsADa0+Q3AbX3t65OcmeRiYCVw53QLlSQNbvEgnZKcDbwW+OW+5huA7UmuAx4BrgGoqt1JtgP3A0eB66vq2IxWLUma1EDhXlXfA541pu1Rep+eGa//FmDLtKuTJJ0Wv6EqSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdNFC4Jzk3yS1JHkzyQJKXJzk/yc4kD7XpeX39NyfZm2RPkqtmr3xJ0ngGvXL/XeDTVfV84EXAA8AmYFdVrQR2tWWSXAKsBy4F1gI3Jlk004VLkiY2ZbgneSbwSuD9AFX1j1X1GLAO2Na6bQOubvPrgJur6khV7QP2AmtmtmxJ0mQGuXJ/LjAKfDDJl5O8L8k5wEVVdRCgTS9s/ZcC+/u2H2ltJ0iyMclwkuHR0dFpnYQk6USDhPti4CXAH1bVi4EnaEMwE8g4bXVSQ9XWqlpdVauHhoYGKlaSNJhBwn0EGKmqO9ryLfTC/lCSJQBteriv//K+7ZcBB2amXEnSIKYM96r6JrA/yfNa0xXA/cAOYENr2wDc1uZ3AOuTnJnkYmAlcOeMVi1JmtTiAfv9e+AjSZ4KfB14C71fDNuTXAc8AlwDUFW7k2yn9wvgKHB9VR2b8colSRMaKNyr6h5g9Tirrpig/xZgy+mXJUmaDr+hKkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHTRQuCd5OMlXk9yTZLi1nZ9kZ5KH2vS8vv6bk+xNsifJVbNVvCRpfKdy5f7qqlpVVccft7cJ2FVVK4FdbZkklwDrgUuBtcCNSRbNYM2SpClMZ1hmHbCtzW8Dru5rv7mqjlTVPmAvsGYax5EknaJBw72Azya5K8nG1nZRVR0EaNMLW/tSYH/ftiOt7QRJNiYZTjI8Ojp6etVLksa1eMB+l1fVgSQXAjuTPDhJ34zTVic1VG0FtgKsXr36pPWSpNM30JV7VR1o08PArfSGWQ4lWQLQpodb9xFged/my4ADM1WwJGlqU4Z7knOSPOP4PPA64D5gB7ChddsA3NbmdwDrk5yZ5GJgJXDnTBcuSZrYIMMyFwG3Jjne/6NV9ekkXwK2J7kOeAS4BqCqdifZDtwPHAWur6pjs1K9JGlcU4Z7VX0deNE47Y8CV0ywzRZgy7SrkySdFr+hKkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHTRwuCdZlOTLST7Vls9PsjPJQ216Xl/fzUn2JtmT5KrZKFySNLFTuXJ/G/BA3/ImYFdVrQR2tWWSXAKsBy4F1gI3Jlk0M+VKkgYxULgnWQa8AXhfX/M6YFub3wZc3dd+c1Udqap9wF5gzYxUK0kayKBX7u8Ffh34YV/bRVV1EKBNL2ztS4H9ff1GWtsJkmxMMpxkeHR09FTrliRNYspwT/JG4HBV3TXgPjNOW53UULW1qlZX1eqhoaEBdy1JGsTiAfpcDrwpyeuBs4BnJvkwcCjJkqo6mGQJcLj1HwGW922/DDgwk0VLkiY35ZV7VW2uqmVVtYLeG6Wfq6o3AzuADa3bBuC2Nr8DWJ/kzCQXAyuBO2e8cknShAa5cp/IDcD2JNcBjwDXAFTV7iTbgfuBo8D1VXVs2pVKkgZ2SuFeVZ8HPt/mHwWumKDfFmDLNGuTJJ0mv6EqSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdNGW4JzkryZ1J7k2yO8m7W/v5SXYmeahNz+vbZnOSvUn2JLlqNk9AknSyQa7cjwCvqaoXAauAtUkuAzYBu6pqJbCrLZPkEnoP0r4UWAvcmGTRLNQuSZrAlOFePd9ti2e0nwLWAdta+zbg6ja/Dri5qo5U1T5gL7BmJouWJE1uoDH3JIuS3AMcBnZW1R3ARVV1EKBNL2zdlwL7+zYfaW1j97kxyXCS4dHR0WmcgiRprIHCvaqOVdUqYBmwJskLJume8XYxzj63VtXqqlo9NDQ0ULGSpMGc0qdlquox4PP0xtIPJVkC0KaHW7cRYHnfZsuAA9MtVJI0uEE+LTOU5Nw2/zTgSuBBYAewoXXbANzW5ncA65OcmeRiYCVw5wzXLUmaxOIB+iwBtrVPvDwF2F5Vn0ryRWB7kuuAR4BrAKpqd5LtwP3AUeD6qjo2O+VLksYzZbhX1VeAF4/T/ihwxQTbbAG2TLs6SdJp8RuqktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYM8Q3V5kr9M8kCS3Une1trPT7IzyUNtel7fNpuT7E2yJ8lVs3kCkqSTDXLlfhR4R1X9FHAZcH2SS4BNwK6qWgnsasu0deuBS4G1wI3t+auSpCfJlOFeVQer6u42/w/AA8BSYB2wrXXbBlzd5tcBN1fVkaraB+wF1sxw3ZKkSZzSmHuSFfQeln0HcFFVHYTeLwDgwtZtKbC/b7OR1jZ2XxuTDCcZHh0dPY3SJUkTGTjckzwd+Djw9qp6fLKu47TVSQ1VW6tqdVWtHhoaGrQMSdIABgr3JGfQC/aPVNUnWvOhJEva+iXA4dY+Aizv23wZcGBmypUkDWKQT8sEeD/wQFW9p2/VDmBDm98A3NbXvj7JmUkuBlYCd85cyZKkqSweoM/lwL8Gvprkntb2TuAGYHuS64BHgGsAqmp3ku3A/fQ+aXN9VR2b6cIlSRObMtyr6q8Zfxwd4IoJttkCbJlGXZKkafAbqpLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGGuyR1kOEuSR1kuEtSBxnuktRBhrskdZDhLkkdZLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EGDPEP1A0kOJ7mvr+38JDuTPNSm5/Wt25xkb5I9Sa6arcIlSRMb5Mr9Q8DaMW2bgF1VtRLY1ZZJcgmwHri0bXNjkkUzVq0kaSBThntV/V/g78c0rwO2tfltwNV97TdX1ZGq2gfsBdbMTKmSpEGd7pj7RVV1EKBNL2ztS4H9ff1GWttJkmxMMpxkeHR09DTLkCSNZ6bfUM04bTVex6raWlWrq2r10NDQDJchST/eTjfcDyVZAtCmh1v7CLC8r98y4MDplydJOh2nG+47gA1tfgNwW1/7+iRnJrkYWAncOb0SJUmnavFUHZLcBLwKuCDJCPAu4AZge5LrgEeAawCqaneS7cD9wFHg+qo6Nku1S5ImMGW4V9W1E6y6YoL+W4At0ylKkjQ9fkNVkjrIcJekDjLcJamDDHdJ6iDDXZI6yHCXpA4y3CWpgwx3Seogw12SOshwl6QOMtwlqYMMd0nqoClvHKaJrdj0Z3Ny3IdveMOcHFfSwuGVuyR1kOEuSR1kuEtSBxnuktRBsxbuSdYm2ZNkb5JNs3UcSdLJZuXTMkkWAX8AvBYYAb6UZEdV3T8bx/txM1ef0gE/qSMtFLP1Ucg1wN6q+jpAkpuBdfQenK0F7Mfx458/jueshW+2wn0psL9veQT46f4OSTYCG9vid5PsmcbxLgC+NY3tnyzWeZry38Ztnnd1TuKUa53gnGfbQnlNrbPnOROtmK1wzzhtdcJC1VZg64wcLBmuqtUzsa/ZZJ0za6HUCQunVuucWXNZ52y9oToCLO9bXgYcmKVjSZLGmK1w/xKwMsnFSZ4KrAd2zNKxJEljzMqwTFUdTfLvgM8Ai4APVNXu2ThWMyPDO08C65xZC6VOWDi1WufMmrM6U1VT95IkLSh+Q1WSOshwl6QOWtDhPp9vcZDkA0kOJ7mvr+38JDuTPNSm581xjcuT/GWSB5LsTvK2+Vhnq+msJHcmubfV+u75Wiv0vqWd5MtJPtWW512dSR5O8tUk9yQZnq91AiQ5N8ktSR5sf19fPt9qTfK89loe/3k8ydvnqs4FG+59tzj4OeAS4Nokl8xtVSf4ELB2TNsmYFdVrQR2teW5dBR4R1X9FHAZcH17DedbnQBHgNdU1YuAVcDaJJcxP2sFeBvwQN/yfK3z1VW1qu+z2PO1zt8FPl1VzwdeRO+1nVe1VtWe9lquAl4KfA+4lbmqs6oW5A/wcuAzfcubgc1zXdeYGlcA9/Ut7wGWtPklwJ65rnFMvbfRux/QfK/zbOBuet96nne10vtexy7gNcCn5uufPfAwcMGYtvlY5zOBfbQPgMznWvtqex3whbmsc8FeuTP+LQ6WzlEtg7qoqg4CtOmFc1zPP0myAngxcAfztM421HEPcBjYWVXztdb3Ar8O/LCvbT7WWcBnk9zVbgcC87PO5wKjwAfbUNf7kpzD/Kz1uPXATW1+TupcyOE+5S0ONJgkTwc+Dry9qh6f63omUlXHqvdf3mXAmiQvmOOSTpLkjcDhqrprrmsZwOVV9RJ6Q5vXJ3nlXBc0gcXAS4A/rKoXA08wf4aLTtK+uPkm4GNzWcdCDveFeIuDQ0mWALTp4TmuhyRn0Av2j1TVJ1rzvKuzX1U9Bnye3nsa863Wy4E3JXkYuBl4TZIPM//qpKoOtOlhemPDa5iHddL7tz7S/qcGcAu9sJ+PtULvl+XdVXWoLc9JnQs53BfiLQ52ABva/AZ6Y9xzJkmA9wMPVNV7+lbNqzoBkgwlObfNPw24EniQeVZrVW2uqmVVtYLe38nPVdWbmWd1JjknyTOOz9MbI76PeVYnQFV9E9if5Hmt6Qp6tw+fd7U21/KjIRmYqzrn+o2Hab5p8Xrga8DfAv9prusZU9tNwEHgB/SuPK4DnkXvjbaH2vT8Oa7xFfSGsr4C3NN+Xj/f6my1vhD4cqv1PuC/tPZ5V2tfza/iR2+ozqs66Y1j39t+dh//9zPf6uyrdxUw3P78PwmcNx9rpfdm/6PAT/S1zUmd3n5AkjpoIQ/LSJImYLhLUgcZ7pLUQYa7JHWQ4S5JHWS4S1IHGe6S1EH/H/svlmrlfcZyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True     197\n",
       "False    197\n",
       "Name: reviews.twoPlusHelpful, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "kindle_reviews = pd.read_csv('kindle_reviews.csv', encoding = 'cp1252') # needed to change encoding because the file was not UTF-8 encoded\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Exploratory data analysis\n",
    "kindle_reviews.shape\n",
    "kindle_reviews.isnull().sum()\n",
    "kindle_reviews[['reviews.doRecommend', 'reviews.rating', 'reviews.numHelpful']].describe()\n",
    "kindle_reviews[['reviews.doRecommend', 'reviews.rating', 'reviews.numHelpful']].info()\n",
    "kindle_reviews['reviews.doRecommend'].value_counts()\n",
    "\n",
    "# Histograms\n",
    "for c in ['reviews.rating', 'reviews.numHelpful']:\n",
    "    kindle_reviews[c].value_counts()\n",
    "    plt.hist(kindle_reviews[c])\n",
    "    plt.title(c)\n",
    "    plt.show()\n",
    "plt.bar(x = np.array([True, False]),\n",
    "        height = kindle_reviews['reviews.doRecommend'].value_counts(),\n",
    "        tick_label = np.array([True, False]))\n",
    "plt.title('reviews.doRecommend')\n",
    "plt.show()\n",
    "\n",
    "# Create categorical outcome variables, balance classes, and prepare to cross-validate\n",
    "kindle_reviews['reviews.fiveStarRating'] = kindle_reviews['reviews.rating'] == 5\n",
    "kindle_reviews['reviews.twoPlusHelpful'] = kindle_reviews['reviews.numHelpful'] >= 2\n",
    "folds = 5\n",
    "kindle_reviews['fold'] = np.random.randint(low = 0, high = folds, size = len(kindle_reviews))\n",
    "\n",
    "# Create up-sampled and down-sampled data\n",
    "kindle_reviews_upsampled_ratings = pd.concat([kindle_reviews[kindle_reviews['reviews.fiveStarRating'] == True], \n",
    "                                              resample(kindle_reviews[kindle_reviews['reviews.fiveStarRating'] == False],\n",
    "                                                       replace = True,\n",
    "                                                       n_samples = len(kindle_reviews[kindle_reviews['reviews.fiveStarRating'] == True]),\n",
    "                                                       random_state = 1234)])\n",
    "kindle_reviews_downsampled_ratings = pd.concat([kindle_reviews[kindle_reviews['reviews.fiveStarRating'] == False], \n",
    "                                                resample(kindle_reviews[kindle_reviews['reviews.fiveStarRating'] == True],\n",
    "                                                         replace = False,\n",
    "                                                         n_samples = len(kindle_reviews[kindle_reviews['reviews.fiveStarRating'] == False]),\n",
    "                                                         random_state = 1234)])\n",
    "kindle_reviews_upsampled_helpful = pd.concat([kindle_reviews[kindle_reviews['reviews.twoPlusHelpful'] == False],\n",
    "                                              resample(kindle_reviews[kindle_reviews['reviews.twoPlusHelpful'] == True],\n",
    "                                                      replace = True,\n",
    "                                                      n_samples = len(kindle_reviews[kindle_reviews['reviews.twoPlusHelpful'] == False]),\n",
    "                                                      random_state = 1234)])\n",
    "kindle_reviews_downsampled_helpful = pd.concat([kindle_reviews[kindle_reviews['reviews.twoPlusHelpful'] == True],\n",
    "                                                resample(kindle_reviews[kindle_reviews['reviews.twoPlusHelpful'] == False],\n",
    "                                                        replace = False,\n",
    "                                                        n_samples = len(kindle_reviews[kindle_reviews['reviews.twoPlusHelpful'] == True]),\n",
    "                                                        random_state = 1234)])\n",
    "kindle_reviews_downsampled_helpful['reviews.twoPlusHelpful'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "Prepare the text of the reviews in the reviews.text field for analysis by eliminating stopwords. What are the top 10 most frequent words? What are the top 10 nouns? What are the top 10 adjectives? (3 points)\n",
    "- I cleaned the text by: \n",
    "    - Eliminating stopwords (using `nltk`'s built-in English stopwords and adding other use-case-specific terms)\n",
    "    - Stemming (using the [Porter stemmer](https://www.nltk.org/howto/stem.html)) \n",
    "    - Lemmatizing (using the [WordNet lemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)\n",
    "- The top 10 most frequent words (after lemmatizing and removing stopwords) are: \n",
    "    - read\n",
    "    - light\n",
    "    - love\n",
    "    - use\n",
    "    - great\n",
    "    - screen\n",
    "    - one\n",
    "    - easy\n",
    "    - page\n",
    "    - book\n",
    "- The top 10 nouns are:\n",
    "    - use\n",
    "    - page\n",
    "    - book\n",
    "    - reader\n",
    "    - screen\n",
    "    - easi\n",
    "    - love\n",
    "    - button\n",
    "    - paperwhit\n",
    "    - read\n",
    "- The top 10 adjectives are:\n",
    "    - great\n",
    "    - light\n",
    "    - best\n",
    "    - read\n",
    "    - new\n",
    "    - good\n",
    "    - much\n",
    "    - screen\n",
    "    - nice\n",
    "    - upgrad\n",
    "- **Note**: The same word can be classified as multiple parts of speech. As such, the overall counts of most frequent words are agnostic to the part of speech and may differ from the top 10 noun and adjective counts.\n",
    "    - For the noun and adjective counts, I only counted the instances in which a given word was tagged as that given part of speech. That is, if the word \"refuse\" appeared once as a verb and once as a noun, I would have counted it only once in looking for the 10 nouns with the highest frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>read</th>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>light</th>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use</th>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen</th>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easi</th>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Freq\n",
       "Word        \n",
       "read     459\n",
       "light    296\n",
       "love     287\n",
       "use      265\n",
       "great    237\n",
       "screen   232\n",
       "one      213\n",
       "easi     205\n",
       "page     204\n",
       "book     194"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "word       part_of_speech\n",
       "use        NN                218\n",
       "page       NN                199\n",
       "book       NN                194\n",
       "reader     NN                152\n",
       "screen     NN                149\n",
       "easi       NN                138\n",
       "love       NN                127\n",
       "button     NN                111\n",
       "paperwhit  NN                107\n",
       "read       NN                102\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "word    part_of_speech\n",
       "great   JJ                237\n",
       "light   JJ                168\n",
       "best    JJS               130\n",
       "read    JJ                128\n",
       "new     JJ                115\n",
       "good    JJ                 82\n",
       "much    JJ                 82\n",
       "nice    JJ                 77\n",
       "screen  JJ                 76\n",
       "upgrad  JJ                 69\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import modules\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import re\n",
    "text = kindle_reviews['reviews.text']\n",
    "cleaned_text = []\n",
    "stop_words = nltk.corpus.stopwords.words('english') + ['amazon', 'kindle', \"'s\", \n",
    "                                                       'voyage', 'nt', 'would']\n",
    "\n",
    "# Remove stopwords and stem/lemmatize\n",
    "def stemmed_sentence(sentence):\n",
    "    # Tokenize sentence\n",
    "    tokenized_words = nltk.tokenize.word_tokenize(sentence.lower())\n",
    "    # Identify part of speech https://www.nltk.org/book/ch05.html\n",
    "    pos = nltk.pos_tag(tokenized_words)\n",
    "    # Initialize empty list\n",
    "    stemmed_sentence = []\n",
    "    # Loop\n",
    "    for word in tokenized_words:\n",
    "        # Ignore/remove stopwords\n",
    "        if word in stop_words or len(word) < 2: \n",
    "            pass\n",
    "        else:\n",
    "            # Use Porter stemming method (or lemmatize?)\n",
    "            word_clean = nltk.stem.PorterStemmer().stem(re.sub(\"[\\.\\-']\", '', word))\n",
    "            # Lemmatize\n",
    "            word_clean = nltk.stem.WordNetLemmatizer().lemmatize(word_clean)\n",
    "            # Append to list\n",
    "            stemmed_sentence.append(word_clean)\n",
    "            stemmed_sentence.append(' ')\n",
    "    return ''.join(stemmed_sentence)\n",
    "\n",
    "kindle_reviews['reviews.textClean'] = [stemmed_sentence(kindle_reviews['reviews.text'][t]) for t in range(len(kindle_reviews['reviews.text']))]\n",
    "\n",
    "# Top 10 words\n",
    "top10_words = (kindle_reviews['reviews.textClean']\n",
    "               .str.lower()\n",
    "               .str.cat()\n",
    "               .split()\n",
    ")\n",
    "display(pd.DataFrame(Counter(top10_words).most_common(10),\n",
    "                     columns=['Word', 'Freq']).set_index('Word'))\n",
    "\n",
    "# Tag by part of speech\n",
    "part_of_speech = pd.DataFrame()\n",
    "for i in range(len(kindle_reviews)): \n",
    "    tokenized_words = nltk.tokenize.word_tokenize(kindle_reviews['reviews.textClean'][i].lower())\n",
    "    pos = nltk.pos_tag(tokenized_words)\n",
    "    part_of_speech = pd.concat([part_of_speech, \n",
    "                                pd.DataFrame({'word': [pos[i][0].strip() for i in range(len(pos))],\n",
    "                                              'part_of_speech': [pos[i][1].strip() for i in range(len(pos))]})])\n",
    "\n",
    "# Top total\n",
    "part_of_speech.value_counts().head(10) # POS definitions here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "# Top nouns\n",
    "display(part_of_speech[part_of_speech.part_of_speech.str.contains('^N') == True].\n",
    " value_counts().head(10))\n",
    "\n",
    "# Top adjectives\n",
    "display(part_of_speech[part_of_speech.part_of_speech.str.contains('^J') == True].\n",
    " value_counts().head(10))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "What are the top 10 most frequent words in reviews that do not recommend purchase of the Kindle? (3 points)\n",
    "- The top 10 most frequent words in reviews that do not recommend purchasing the Kindle are:\n",
    "    - not\n",
    "    - device\n",
    "    - turn\n",
    "    - light\n",
    "    - page\n",
    "    - buy\n",
    "    - one\n",
    "    - screen\n",
    "    - like\n",
    "    - read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nt</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>devic</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>turn</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>light</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buy</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>read</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Freq\n",
       "Word        \n",
       "nt        17\n",
       "devic     14\n",
       "turn      13\n",
       "light     10\n",
       "page       9\n",
       "buy        8\n",
       "one        8\n",
       "screen     7\n",
       "like       7\n",
       "read       7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top 10 words when not recommended\n",
    "top10_words_no_rec = (kindle_reviews[kindle_reviews['reviews.doRecommend'] == False]['reviews.textClean']\n",
    "                      .str.lower()\n",
    "                      .str.cat()\n",
    "                      .split())\n",
    "display(pd.DataFrame(Counter(top10_words_no_rec).most_common(10),\n",
    "                     columns=['Word', 'Freq']).set_index('Word'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "Create a model that will predict when a customer will give the Kindle a 5-star rating based on the text of the review. Evaluate the accuracy of your model. (3 points)\n",
    "- I followed the guidance of the NLTK documentation (https://www.nltk.org/book/ch06.html) and created a list of `tuple`s with two elements: \n",
    "    - The tokenized words in the review (after stemming and lemmatization above)\n",
    "    - A boolean that indicated whether the review gave the product five stars\n",
    "- I then created a feature extractor by:\n",
    "    - Creating a list of the unique words that appear in all reviews (including both five-star and non-five-star reviews and across both the training and test sets)\n",
    "    - Defining a function that checks for each word in the list of unique words whether the word appears in the review\n",
    "    - Using that function to create feature sets (which indicate both which words appeared and whether the review was a five-star review or not)\n",
    "- I split the data into training and test cases (60/40) and fit a model using the `NaiveBayesClassifier`.\n",
    "- Below are the measures of the model's accuracy: \n",
    "    - Accuracy: 72%\n",
    "    - Recall: 31%\n",
    "    - Precision: 58%\n",
    "    - **Note**: You may notice slightly different numbers when running the code because of the `random.shuffle(documents)` code snippet (which apparently does not pay attention to the `np.random.seed(1234)` set in the first code chunk.\n",
    "- Because this model did not perform particularly well on the test data, I decided to re-run the model with down-sampled and up-sampled data.\n",
    "    - **Note**: The results of the model trained on down-sampled data were similarly poor, presumably because there were so many fewer data points, so I have omitted those results here.\n",
    "    - This model performed better on the test data: \n",
    "        - Accuracy: 80%\n",
    "        - Recall: 92%\n",
    "        - Precision: 74%\n",
    "    - The most informative features (i.e., those words whose presence was most closely linked to either five-star or not five-star reviews) included: \n",
    "        - return: 13 times more likely to appear in reviews with less than five stars\n",
    "        - give: 10 times more likely to appear in reviews with less than five stars\n",
    "        - problem: 9 times more likely to appear in reviews with less than five stars\n",
    "        - own: 8 times more likely to appear in five-star reviews\n",
    "        - service: 8 times more likely to appear in reviews with less than five stars\n",
    "        - **Note**: Again, these features and estimates may differ slightly based on the random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import random\n",
    "\n",
    "# Create nested list of tokenized words and whether the text was for a 5-star review\n",
    "documents = [(nltk.tokenize.word_tokenize(kindle_reviews['reviews.textClean'][i]),\n",
    "              kindle_reviews['reviews.fiveStarRating'][i]) \n",
    "              for i in range(len(kindle_reviews))]\n",
    "random.shuffle(documents) # randomly reorder to create training/test splits\n",
    "\n",
    "# Create list of distinct words in reviews.textClean\n",
    "word_features = (list(set(kindle_reviews['reviews.textClean'].str.cat().split())))\n",
    "\n",
    "# Define function to extract features\n",
    "def document_features(document): \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Define feature sets\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "\n",
    "# Create training and test set and train model\n",
    "split = int(len(featuresets) * 0.6)\n",
    "train_set, test_set = featuresets[split:], featuresets[:split]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Calculate model accuracy and determine most informative features\n",
    "print('Model accuracy:', nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(10)\n",
    "\n",
    "# Retry with up-sampled and down-sampled datasets\n",
    "upsampled = pd.merge(kindle_reviews_upsampled_ratings,\n",
    "                     kindle_reviews[['reviews.textClean']],\n",
    "                     how = 'left', left_index = True, right_index = True).reset_index()\n",
    "downsampled = pd.merge(kindle_reviews_downsampled_ratings,\n",
    "                      kindle_reviews[['reviews.textClean']],\n",
    "                      how = 'left', left_index = True, right_index = True).reset_index()\n",
    "# Downsampled\n",
    "'''\n",
    "documents = [(nltk.tokenize.word_tokenize(downsampled['reviews.textClean'][i]),\n",
    "              downsampled['reviews.fiveStarRating'][i]) \n",
    "             for i in range(len(downsampled))]\n",
    "random.shuffle(documents) # randomly reorder to create training/test splits\n",
    "word_features = (list(set(downsampled['reviews.textClean'].str.cat().split())))\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "split = int(len(featuresets) * 0.6)\n",
    "train_set, test_set = featuresets[split:], featuresets[:split]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "# Re-calculate model accuracy and determine most informative features\n",
    "print('Model accuracy (downsampled):', nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(10)\n",
    "'''\n",
    "# Upsampled\n",
    "documents_upsampled = [(nltk.tokenize.word_tokenize(upsampled['reviews.textClean'][i]),\n",
    "                        upsampled['reviews.fiveStarRating'][i]) \n",
    "                       for i in range(len(upsampled))]\n",
    "random.shuffle(documents_upsampled) # randomly reorder to create training/test splits\n",
    "word_features = (list(set(upsampled['reviews.textClean'].str.cat().split())))\n",
    "featuresets_upsampled = [(document_features(d), c) for (d,c) in documents_upsampled]\n",
    "split_upsampled = int(len(featuresets) * 0.6)\n",
    "train_set_upsampled, test_set_upsampled = featuresets_upsampled[split_upsampled:], featuresets_upsampled[:split_upsampled]\n",
    "classifier_upsampled = nltk.NaiveBayesClassifier.train(train_set)\n",
    "# Re-calculate model accuracy and determine most informative features\n",
    "print('Model accuracy (up-sampled):', nltk.classify.accuracy(classifier_upsampled, test_set_upsampled))\n",
    "classifier_upsampled.show_most_informative_features(10)\n",
    "\n",
    "# Confusion matrix\n",
    "nltk.ConfusionMatrix([l for (f, l) in test_set_upsampled],\n",
    "                     classifier_upsampled.classify_many([f for (f, l) in test_set_upsampled]))\n",
    "predictions = pd.DataFrame({'actual':[l for (f, l) in test_set],\n",
    "                            'predicted':classifier.classify_many([f for (f, l) in test_set])})\n",
    "predictions_upsampled = pd.DataFrame({'actual':[l for (f, l) in test_set_upsampled],\n",
    "                                      'predicted':classifier_upsampled.classify_many([f for (f, l) in test_set_upsampled])})\n",
    "# Unaltered data\n",
    "tp = predictions.query('actual == True & predicted == True').count()[0]\n",
    "fp = predictions.query('actual == False & predicted == True').count()[0]\n",
    "tn = predictions.query('actual == False & predicted == False').count()[0]\n",
    "fn = predictions.query('actual == True & predicted == False').count()[0]\n",
    "# Up-sampled data\n",
    "tp_upsampled = predictions_upsampled.query('actual == True & predicted == True').count()[0]\n",
    "fp_upsampled = predictions_upsampled.query('actual == False & predicted == True').count()[0]\n",
    "tn_upsampled = predictions_upsampled.query('actual == False & predicted == False').count()[0]\n",
    "fn_upsampled = predictions_upsampled.query('actual == True & predicted == False').count()[0]\n",
    "\n",
    "# Precision - what proportion of predicted positive values were actually positive?\n",
    "# (true positive / (true positive + false positive))\n",
    "precision = tp / (tp + fp); precision_upsampled = tp_upsampled / (tp_upsampled + fp_upsampled); \n",
    "# Recall - what proportion of actual positive value were predicted to be positive? \n",
    "recall = tp / (tp + fn); recall_upsampled = tp_upsampled / (tp_upsampled + fn_upsampled)\n",
    "\n",
    "print('Recall:', recall, '\\nPrecision:', precision)\n",
    "print('Recall (up-sampled):', recall_upsampled, '\\nPrecision (up-sampled)', precision_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5\n",
    "Create a model that will predict when at least two customers will find a review helpful. Evaluate the accuracy of your model. (3 points)\n",
    "- Tested two models: Naive Bayes Classifier and Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create nested list of tokenized words and whether the text was for a 5-star review\n",
    "classifier_naive = nltk.NaiveBayesClassifier.train(train_set)\n",
    "classifier_tree = nltk.DecisionTreeClassifier.train(train_set)\n",
    "\n",
    "# Calculate model accuracy and determine most informative features\n",
    "print('Model accuracy (Naive Bayes):', nltk.classify.accuracy(classifier_naive, test_set))\n",
    "print('Model accuracy (Decision Tree):', nltk.classify.accuracy(classifier_tree, test_set))\n",
    "classifier_naive.show_most_informative_features(10)\n",
    "# classifier_tree.show_most_informative_features(10)\n",
    "\n",
    "# Predictions\n",
    "predictions_naive = pd.DataFrame({'actual':[l for (f, l) in test_set],\n",
    "                                 'predicted':classifier_naive.classify_many([f for (f, l) in test_set])})\n",
    "predictions_tree = pd.DataFrame({'actual':[l for (f, l) in test_set],\n",
    "                                 'predicted':classifier_tree.classify_many([f for (f, l) in test_set])})\n",
    "tp_naive = predictions_naive.query('actual == True & predicted == True').count()[0]\n",
    "fp_naive = predictions_naive.query('actual == False & predicted == True').count()[0]\n",
    "tn_naive = predictions_naive.query('actual == False & predicted == False').count()[0]\n",
    "fn_naive = predictions_naive.query('actual == True & predicted == False').count()[0]\n",
    "tp_tree = predictions_tree.query('actual == True & predicted == True').count()[0]\n",
    "fp_tree = predictions_tree.query('actual == False & predicted == True').count()[0]\n",
    "tn_tree = predictions_tree.query('actual == False & predicted == False').count()[0]\n",
    "fn_tree = predictions_tree.query('actual == True & predicted == False').count()[0]\n",
    "\n",
    "# Precision and recall\n",
    "precision_naive = tp_naive / (tp_naive + fp_naive); recall_naive = tp_naive / (tp_naive + fn_naive)\n",
    "precision_tree = tp_tree / (tp_tree + fp_tree); recall_tree = tp_tree / (tp_tree + fn_tree)\n",
    "print('Recall (Naive Bayes):', recall_naive, '\\nPrecision (Naive Bayes):', precision_naive)\n",
    "print('Recall (Decision Tree):', recall_tree, '\\nPrecision (Naive Bayes):', precision_tree)\n",
    "\n",
    "\n",
    "# Retry with up-sampled data?\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "# AUROC\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
